{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:30.038042Z",
     "start_time": "2025-03-04T05:43:30.008502Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wilcoxon\n",
    "import scipy.stats as stats\n",
    "from datetime import timedelta, datetime\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf61bf701a265f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:30.209905Z",
     "start_time": "2025-03-04T05:43:30.205539Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/bugragorkem/Desktop/Uni/5. Semester /Data Science Projekt/archive/p_data/p_einzelnt/'\n",
    "MONTHS = [\"p01\", \"p02\", \"p03\", \"p04\", \"p05\", \"p06\", \"p07\", \"p08\", \"p09\", \"p10\", \"p11\", \"p12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968aad9e94b71f59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:30.323561Z",
     "start_time": "2025-03-04T05:43:30.319732Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorize_incident(description):\n",
    "    if description in [\"1144-Fatality\", \"YELLOW-Hit and Run Fatality\", \"1180-Trfc Collision-Major Inj\"]:\n",
    "        return \"tote\"\n",
    "    elif description in [\"1182-Trfc Collision-No Inj\", \"20002-Hit and Run No Injuries\"]:\n",
    "        return \"keine_verletzte\"\n",
    "    elif description in [\"20001-Hit and Run w/Injuries\", \"1181-Trfc Collision-Minor Inj\"]:\n",
    "        return \"verletzte\"\n",
    "    else:\n",
    "        return \"unbekannt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad7933b10d5f14da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:30.527109Z",
     "start_time": "2025-03-04T05:43:30.522387Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_closest_sensor(incidence_df, sensor_df, max_distance=0.3): \n",
    "    # Ensure the ABS_PM columns are sorted for faster searching\n",
    "    sensor_df = sensor_df.sort_values(by=['Fwy', 'Abs PM'])\n",
    "\n",
    "    closest_sensors = []\n",
    "\n",
    "    for _, incident in incidence_df.iterrows():\n",
    "        fwy = incident['Fwy']\n",
    "        abs_pm = incident['Abs PM']\n",
    "\n",
    "        # Filter sensors for the same freeway\n",
    "        matching_sensors = sensor_df[(sensor_df['Fwy'] == fwy) & (sensor_df['Type'] == 'HOV' )]\n",
    "\n",
    "        # Find the closest preceding sensor within the max distance\n",
    "        preceding_sensors = matching_sensors[\n",
    "            (matching_sensors['Abs PM'] <= abs_pm) & \n",
    "            (abs_pm - matching_sensors['Abs PM'] <= max_distance)\n",
    "        ]\n",
    "\n",
    "        if not preceding_sensors.empty:\n",
    "            closest_sensor = preceding_sensors.iloc[-1]  # Last one is the closest preceding sensor\n",
    "            closest_sensors.append({\n",
    "                'Incident_ABS_PM': abs_pm,\n",
    "                'Incident_Fwy': fwy,\n",
    "                \"station_id\": closest_sensor['station_id'], \n",
    "                'Sensor_ABS_PM': closest_sensor['Abs PM'],\n",
    "                'Sensor_Fwy': closest_sensor['Fwy'],\n",
    "                'time': incident['dt'],\n",
    "                'Distance': abs_pm - closest_sensor['Abs PM']\n",
    "            })\n",
    "\n",
    "\n",
    "    return pd.DataFrame(closest_sensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4addaa3aa43d0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:30.690066Z",
     "start_time": "2025-03-04T05:43:30.685868Z"
    }
   },
   "outputs": [],
   "source": [
    "def cliffs_delta(x, y):\n",
    "    \"\"\"\n",
    "    Berechnet Cliff's Delta als Effektstärkemaß.\n",
    "    \n",
    "    Parameter:\n",
    "    x, y : array-like\n",
    "        Die beiden Samples, zwischen denen der Effekt gemessen werden soll.\n",
    "    \n",
    "    Rückgabe:\n",
    "    delta : float\n",
    "        Der berechnete Cliff's Delta Wert.\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    n1 = len(x)\n",
    "    n2 = len(y)\n",
    "    # Vergleiche alle Paare (x, y)\n",
    "    wins = np.sum(np.greater.outer(x, y))\n",
    "    losses = np.sum(np.less.outer(x, y))\n",
    "    delta = (wins - losses) / (n1 * n2)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2629320d9e32558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:30.888150Z",
     "start_time": "2025-03-04T05:43:30.885488Z"
    }
   },
   "outputs": [],
   "source": [
    "def datetime_to_timeslot(dt, month_start):\n",
    "    \"\"\"Konvertiert datetime zu Timeslot-Index relativ zum Monatsanfang\"\"\"\n",
    "    delta = dt - month_start\n",
    "    return int(delta.total_seconds() // 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7a9738ac1c908e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:31.095559Z",
     "start_time": "2025-03-04T05:43:31.091897Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_sensors_to_indices(sensor_df, monthly_station_ids):\n",
    "    # Sicherstellen, dass der Datentyp von `station_id` mit `monthly_station_ids` übereinstimmt\n",
    "    sensor_df['station_id'] = sensor_df['station_id'].astype(str)\n",
    "    monthly_station_ids = [str(station_id) for station_id in monthly_station_ids]\n",
    "    \n",
    "    #Erstelle ein Mapping von `station_id` zu den entsprechenden Indizes\n",
    "    sensor_to_index = {station_id: i for i, station_id in enumerate(monthly_station_ids)}\n",
    "    \n",
    "    # Füge eine neue Spalte mit den Indizes hinzu\n",
    "    sensor_df['Index 12'] = sensor_df['station_id'].map(sensor_to_index)\n",
    "    return sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f19f51dde34649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:31.255488Z",
     "start_time": "2025-03-04T05:43:31.252517Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_traffic_data(npy_data, sensor_indices, time_window=1):\n",
    "    extracted_data = {}\n",
    "    for incident_time, sensor_index in sensor_indices:\n",
    "        start_time = max(0, incident_time - time_window)  # Zeit vor dem Unfall\n",
    "        end_time = incident_time + time_window            # Zeit nach dem Unfall\n",
    "\n",
    "        # Extrahiere Daten für die jeweilige Zeitspanne\n",
    "        extracted_data[(incident_time, sensor_index)] = npy_data[sensor_index, start_time:end_time]\n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0ef6e9fb7aec7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:31.599805Z",
     "start_time": "2025-03-04T05:43:31.596900Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_month_data(month):\n",
    "    \"\"\"Lädt Daten für einen bestimmten Monat\"\"\"\n",
    "    traffic_data = np.load(os.path.join(DATA_PATH, f'{month}_merged.npy'))\n",
    "    station_ids = np.load(os.path.join(DATA_PATH, f'{month}_node_order.npy'))\n",
    "    return traffic_data, station_ids.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "520250f30fda99ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:41.695148Z",
     "start_time": "2025-03-04T05:43:40.966046Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/bugragorkem/Desktop/Uni/5. Semester /Data Science Projekt/archive/incidents.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Lese Unfalldaten ein\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m incidents \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/bugragorkem/Desktop/Uni/5. Semester /Data Science Projekt/archive/incidents.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m incidents[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m incidents[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mto_period(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Lese Sensormetadaten ein\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/bugragorkem/Desktop/Uni/5. Semester /Data Science Projekt/archive/incidents.csv'"
     ]
    }
   ],
   "source": [
    "# Lese Unfalldaten ein\n",
    "incidents = pd.read_csv('/Users/bugragorkem/Desktop/Uni/5. Semester /Data Science Projekt/archive/incidents.csv', \n",
    "                        sep=\"\\t\", parse_dates=['dt'])\n",
    "incidents['month'] = incidents['dt'].dt.to_period('M')\n",
    "\n",
    "# Lese Sensormetadaten ein\n",
    "sensor_meta = pd.read_csv('/Users/bugragorkem/Desktop/Uni/5. Semester /Data Science Projekt/archive/sensor_meta_feature.csv', \n",
    "                          sep=\"\\t\")\n",
    "\n",
    "\n",
    "# Filter: Nur Unfälle und solche mit Duration >= 150 Minuten\n",
    "incidents = incidents[incidents['type'] == \"accident\"]\n",
    "incidents = incidents[incidents['Duration (mins)'] >= 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48afd1fdc075c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T05:43:42.745572Z",
     "start_time": "2025-03-04T05:43:42.181744Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4116078327.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    for x in\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "# Gruppierung der Unfälle nach Injury-Status\n",
    "for x in month:\n",
    "    print(f\"\\n--- Analyse für Injury Status: {status} ---\")\n",
    "    all_data_status = []\n",
    "    \n",
    "    # Gruppierung nach Monat innerhalb des jeweiligen Injury-Status\n",
    "    for month, month_incidents in status_incidents.groupby('month'):\n",
    "        month_str = f\"p{month.month:02d}\"\n",
    "        if month_str not in MONTHS:\n",
    "            continue\n",
    "        \n",
    "        # Versuche, die Verkehrsdaten für den aktuellen Monat zu laden\n",
    "        try:\n",
    "            traffic_data = np.load(os.path.join(DATA_PATH, f'{month_str}_car.npy'))\n",
    "            station_ids = np.load(os.path.join(DATA_PATH, f'{month_str}_car_node.npy')).astype(str)\n",
    "            station_ids = np.delete(station_ids, 0)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Dateien für {month_str} nicht gefunden\")\n",
    "            continue\n",
    "\n",
    "        # Finde die zugehörigen Sensoren für die Unfälle im aktuellen Monat\n",
    "        closest_sensors = find_closest_sensor(month_incidents, sensor_meta)\n",
    "        \n",
    "        # Prüfung: Falls keine Sensoren gefunden wurden, diesen Monat überspringen\n",
    "        if closest_sensors.empty:\n",
    "            print(f\"Keine Sensoren gefunden für Monat {month_str} und Injury Status {status}\")\n",
    "            continue\n",
    "        \n",
    "        station_to_index = {sid: i for i, sid in enumerate(station_ids)}\n",
    "        closest_sensors['sensor_index'] = closest_sensors['station_id'].astype(str).map(station_to_index)\n",
    "        valid_sensors = closest_sensors.dropna(subset=['sensor_index']).copy()\n",
    "        \n",
    "        # Zeitkonvertierung: 5 Minuten vorverschieben und Timeslot relativ zum Monatsanfang berechnen\n",
    "        month_start = datetime(month.year, month.month, 1)\n",
    "        valid_sensors['time'] = pd.to_datetime(valid_sensors['time'], errors='coerce')\n",
    "        valid_sensors['time'] = valid_sensors['time'] - pd.Timedelta(minutes=5)\n",
    "        valid_sensors['timeslot'] = valid_sensors['time'].apply(lambda x: datetime_to_timeslot(x, month_start))\n",
    "        \n",
    "        time_window = 3  # Anzahl der 5-Minuten-Intervalle vor und nach dem Vorfall\n",
    "        for _, row in valid_sensors.iterrows():\n",
    "            idx = int(row['sensor_index'])\n",
    "            ts = row['timeslot']\n",
    "            if ts < 0 or ts >= traffic_data.shape[0]:\n",
    "                continue\n",
    "\n",
    "            start = max(0, ts - time_window)\n",
    "            end = min(traffic_data.shape[0], ts + time_window + 1)  # +1, um inklusiv zu schneiden\n",
    "            if (end - start) < (2 * time_window):\n",
    "                continue\n",
    "\n",
    "            traffic_slice = traffic_data[start:end, idx]\n",
    "            if np.isnan(traffic_slice).any():\n",
    "                continue\n",
    "\n",
    "            all_data_status.append({\n",
    "                'pre': traffic_slice[:time_window],\n",
    "                'post': traffic_slice[time_window+1:],  # Zeitpunkt des Vorfalls überspringen\n",
    "                'feature_names': ['Traffic Volume', 'Occupancy Rate', 'Speed']\n",
    "            })\n",
    "            print(f\"{month_str} für Injury Status {status} verarbeitet\")\n",
    "            \n",
    "    if len(all_data_status) == 0:\n",
    "        print(f\"Keine gültigen Daten für Injury Status {status}\")\n",
    "        continue\n",
    "\n",
    "    # Aggregiere die Pre- und Post-Daten\n",
    "    pre_data = []\n",
    "    post_data = []\n",
    "    for entry in all_data_status:\n",
    "        pre_data.extend(entry['pre'][:])\n",
    "        post_data.extend(entry['post'][:])\n",
    "    \n",
    "    # Optional: Entferne die letzten drei Elemente aus pre_data (wie im Originalcode)\n",
    "    if len(pre_data) > 1:\n",
    "        pre_data.pop()\n",
    "        pre_data.pop()\n",
    "        pre_data.pop()\n",
    "    \n",
    "    min_length = min(len(pre_data), len(post_data))\n",
    "    if min_length == 0:\n",
    "        print(f\"Nicht genügend Daten für Injury Status {status}\")\n",
    "        continue\n",
    "\n",
    "    # Wilcoxon-Test (alternative='greater')\n",
    "    stat, p_value = stats.wilcoxon(pre_data[:min_length], post_data[:min_length], alternative='greater')\n",
    "    # Berechnung von Cliff's Delta\n",
    "    delta = cliffs_delta(pre_data[:min_length], post_data[:min_length])\n",
    "    \n",
    "    all_results[status] = {\n",
    "        'statistic': stat,\n",
    "        'p_value': p_value,\n",
    "        'median_pre': np.median(pre_data[:min_length]),\n",
    "        'median_post': np.median(post_data[:min_length]),\n",
    "        'cliffs_delta': delta\n",
    "    }\n",
    "    \n",
    "    # Konvertiere zu numpy-Arrays und kappe Extremwerte (z. B. 99%-Quantil)\n",
    "    pre_data = np.array(pre_data[:min_length])\n",
    "    post_data = np.array(post_data[:min_length])\n",
    "    pre_data = np.clip(pre_data, 0, np.percentile(pre_data, 99))\n",
    "    post_data = np.clip(post_data, 0, np.percentile(post_data, 99))\n",
    "    \n",
    "    # --- Boxplot Visualization ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.ylim(0, max(pre_data.max(), post_data.max()) * 1.1)\n",
    "    sns.boxplot(data=[pre_data, post_data], palette=[\"#4c72b0\", \"#dd8452\"])\n",
    "    plt.xticks([0, 1], ['Vor Unfall', 'Nach Unfall'])\n",
    "    plt.title(\"\")\n",
    "    plt.ylabel('Verkehrsaufkommen')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Violinplot Visualization ---\n",
    "    data_plot = pd.DataFrame({\n",
    "        'Traffic Volume': np.concatenate([pre_data, post_data]),\n",
    "        'Period': ['Vor Unfall'] * len(pre_data) + ['Nach Unfall'] * len(post_data)\n",
    "    })\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(x='Period', y='Traffic Volume', data=data_plot, palette=[\"#4c72b0\", \"#dd8452\"])\n",
    "    plt.title(f'')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Verkehrsaufkommen')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    n_pre = len(pre_data[:min_length])\n",
    "    n_post = len(post_data[:min_length])\n",
    "    print(f\"Anzahl der Datenpunkte für {status}: Pre = {n_pre}, Post = {n_post}\")\n",
    "\n",
    "print(\"Analyse Ergebnisse:\")\n",
    "print(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c8d9a62ca651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for feature_idx in range(1):  # Für alle 3 Features, hier nur Feature 0 (Traffic Volume)\n",
    "    pre_data = []\n",
    "    post_data = []\n",
    "\n",
    "    for entry in all_data:\n",
    "        pre_data.extend(entry['pre'][:])\n",
    "        post_data.extend(entry['post'][:])\n",
    "\n",
    "    # Entferne das letzte Element von pre_data, falls es nicht leer ist\n",
    "    if len(pre_data) > 1:\n",
    "        pre_data.pop()\n",
    "        pre_data.pop()\n",
    "        pre_data.pop()\n",
    "\n",
    "    print(f\"Feature {feature_idx}: pre_data Länge = {len(pre_data)}, post_data Länge = {len(post_data)}\")\n",
    "    print(f\"pre_data: {pre_data[:10]}\")\n",
    "    print(f\"post_data: {post_data[:10]}\")\n",
    "    print(f\"Einzigartige Werte in pre_data: {len(set(pre_data))}\")\n",
    "    print(f\"Einzigartige Werte in post_data: {len(set(post_data))}\")\n",
    "\n",
    "    # Sicherstellen, dass beide Listen gleich lang sind\n",
    "    min_length = min(len(pre_data), len(post_data))\n",
    "\n",
    "    if min_length > 0:  # Verhindere Fehler durch leere Listen\n",
    "        stat, p_value = stats.wilcoxon(pre_data[:min_length], post_data[:min_length], alternative='greater')\n",
    "\n",
    "        feature_name = entry['feature_names'][feature_idx]  # Feature-Namen korrekt referenzieren\n",
    "        results[feature_name] = {\n",
    "            'statistic': stat,\n",
    "            'p_value': p_value,\n",
    "            'median_pre': np.median(pre_data[:min_length]),\n",
    "            'median_post': np.median(post_data[:min_length])\n",
    "        }\n",
    "\n",
    "        # Konvertiere die Daten in numpy-Arrays\n",
    "        pre_data = np.array(pre_data[:min_length])\n",
    "        post_data = np.array(post_data[:min_length])\n",
    "\n",
    "        # Optional: Extremwerte kappen (z.B. auf das 99%-Quantil)\n",
    "        pre_data = np.clip(pre_data, 0, np.percentile(pre_data, 99))\n",
    "        post_data = np.clip(post_data, 0, np.percentile(post_data, 99))\n",
    "\n",
    "        # --- Boxplot Visualization ---\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.ylim(0, 750)  # oder ein anderes sinnvolles Limit\n",
    "        sns.boxplot(data=[pre_data, post_data], palette=[\"#4c72b0\", \"#dd8452\"])\n",
    "        plt.xticks([0, 1], ['Pre-Incident', 'Post-Incident'])\n",
    "        plt.title(f\"Traffic Volume\\np-value: {p_value:.4f}\")\n",
    "        plt.ylabel('Traffic Volume (Normalized)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "        # --- Violinplot Visualization ---\n",
    "        # Daten in einen DataFrame packen\n",
    "        data = pd.DataFrame({\n",
    "            'Traffic Volume': np.concatenate([pre_data, post_data]),\n",
    "            'Period': ['Pre-Incident'] * len(pre_data) + ['Post-Incident'] * len(post_data)\n",
    "        })\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.violinplot(x='Period', y='Traffic Volume', data=data, palette=[\"#4c72b0\", \"#dd8452\"])\n",
    "        plt.title('Verteilung des Traffic Volume vor und nach dem Unfall')\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('Traffic Volume (Normalized)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.plot\n",
    "\n",
    "# Ausgabe der statistischen Ergebnisse\n",
    "print(\"Statistische Ergebnisse:\")\n",
    "for feature, res in results.items():\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Wilcoxon-Statistik: {res['statistic']}\")\n",
    "    print(f\"  P-Wert: {res['p_value']:.4f}\")\n",
    "    print(f\"  Median vorher: {res['median_pre']:.2f}\")\n",
    "    print(f\"  Median nachher: {res['median_post']:.2f}\")\n",
    "    print(f\"  Signifikant (p < 0.05): {'Ja' if res['p_value'] < 0.05 else 'Nein'}\")\n",
    "\n",
    "print(\"Statistische Ergebnisse:\")\n",
    "for feature, res in results.items():\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Wilcoxon-Statistik: {res['statistic']}\")\n",
    "    print(f\"  P-Wert: {res['p_value']:.4f}\")\n",
    "    print(f\"  Median vorher: {res['median_pre']:.2f}\")\n",
    "    print(f\"  Median nachher: {res['median_post']:.2f}\")\n",
    "    print(f\"  Signifikant (p < 0.05): {'Ja' if res['p_value'] < 0.05 else 'Nein'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6533f5f1486e00e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

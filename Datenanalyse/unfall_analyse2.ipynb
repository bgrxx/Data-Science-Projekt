{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97c82dfbaad2f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T19:19:24.007777Z",
     "start_time": "2025-02-27T19:19:24.004157Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wilcoxon\n",
    "import scipy.stats as stats\n",
    "from datetime import timedelta, datetime\n",
    "import os\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26882591ecb873cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T19:19:24.011868Z",
     "start_time": "2025-02-27T19:19:24.009401Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/bugragorkem/Desktop/Uni/5. Semester /Data Science Projekt/archive/p_data/p_einzelnt/'\n",
    "MONTHS = [\"p01\", \"p02\", \"p03\", \"p04\", \"p05\", \"p06\", \"p07\", \"p08\", \"p09\", \"p10\", \"p11\", \"p12\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "713e81ea831320b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T19:19:24.018535Z",
     "start_time": "2025-02-27T19:19:24.013012Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_closest_sensor(incidence_df, sensor_df, max_distance=0.3, mainline_threshold=0.3): \n",
    "    # Sortiere Sensoren für schnelleres Suchen\n",
    "    sensor_df = sensor_df.sort_values(by=['Fwy', 'Abs PM'])\n",
    "    closest_sensors = []\n",
    "\n",
    "    for _, incident in incidence_df.iterrows():\n",
    "        fwy = incident['Fwy']\n",
    "        abs_pm = incident['Abs PM']\n",
    "\n",
    "        # Suche HOV-Sensor\n",
    "        hov_sensors = sensor_df[(sensor_df['Fwy'] == fwy) & (sensor_df['Type'] == 'HOV')]\n",
    "        hov_preceding = hov_sensors[\n",
    "            (hov_sensors['Abs PM'] <= abs_pm) & \n",
    "            (abs_pm - hov_sensors['Abs PM'] <= max_distance)\n",
    "        ]\n",
    "        if hov_preceding.empty:\n",
    "            continue\n",
    "        hov_sensor = hov_preceding.iloc[-1]\n",
    "\n",
    "        # Suche Mainline-Sensor im selben Fwy\n",
    "        mainline_sensors = sensor_df[(sensor_df['Fwy'] == fwy) & (sensor_df['Type'] == 'mainline')]\n",
    "        if mainline_sensors.empty:\n",
    "            continue\n",
    "        \n",
    "        # Ermittle Abstand vom HOV-Sensor\n",
    "        mainline_sensors = mainline_sensors.copy()\n",
    "        mainline_sensors['distance_to_hov'] = abs(mainline_sensors['Abs PM'] - hov_sensor['Abs PM'])\n",
    "        \n",
    "        # Filter: max. mainline_threshold Meilen Unterschied\n",
    "        mainline_candidates = mainline_sensors[mainline_sensors['distance_to_hov'] <= mainline_threshold]\n",
    "        if mainline_candidates.empty:\n",
    "            continue\n",
    "\n",
    "        # Nimm den nächstgelegenen Mainline-Sensor\n",
    "        mainline_sensor = mainline_candidates.iloc[0]\n",
    "\n",
    "        closest_sensors.append({\n",
    "            'Incident_ABS_PM': abs_pm,\n",
    "            'Incident_Fwy': fwy,\n",
    "            \"hov_station_id\": hov_sensor['station_id'], \n",
    "            'HOV_Sensor_ABS_PM': hov_sensor['Abs PM'],\n",
    "            'mainline_station_id': mainline_sensor['station_id'],\n",
    "            'Mainline_Sensor_ABS_PM': mainline_sensor['Abs PM'],\n",
    "            'time': incident['dt'],\n",
    "            'Distance': abs_pm - hov_sensor['Abs PM']\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(closest_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "604cba7b24810d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T19:19:24.022468Z",
     "start_time": "2025-02-27T19:19:24.019882Z"
    }
   },
   "outputs": [],
   "source": [
    "def datetime_to_timeslot(dt, month_start):\n",
    "    \"\"\"Konvertiert datetime zu einem Timeslot-Index relativ zum Monatsanfang\"\"\"\n",
    "    delta = dt - month_start\n",
    "    return int(delta.total_seconds() // 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "614c832b37bd1e6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T19:19:24.727093Z",
     "start_time": "2025-02-27T19:19:24.026017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lade Vorfall-Daten\n",
    "incidents = pd.read_csv('/Users/bugragorkem/Desktop/Uni/5. Semester /Data Science Projekt/archive/incidents.csv', \n",
    "                         sep=\"\\t\", parse_dates=['dt'])\n",
    "incidents['month'] = incidents['dt'].dt.to_period('M')\n",
    "\n",
    "# Filter: Nur Unfälle und solche mit Duration >= 15 Minuten\n",
    "incidents = incidents[incidents['type'] == \"accident\"]\n",
    "incidents = incidents[incidents['Duration (mins)'] >= 15]\n",
    "\n",
    "# Lade Sensor-Metadaten\n",
    "sensor_meta = pd.read_csv('/Users/bugragorkem/Desktop/Uni/5. Semester /Data Science Projekt/archive/sensor_meta_feature.csv', \n",
    "                          sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118cb8cc8d73554",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-27T19:19:24.728599Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyse für DESCRIPTION: 20002-Hit and Run No Injuries ---\n",
      "Keine gültigen Daten für DESCRIPTION 20002-Hit and Run No Injuries\n",
      "\n",
      "--- Analyse für DESCRIPTION: 1182-Trfc Collision-No Inj ---\n",
      "Keine gültigen Daten für DESCRIPTION 1182-Trfc Collision-No Inj\n",
      "\n",
      "--- Analyse für DESCRIPTION: 1183-Trfc Collision-Unkn Inj ---\n"
     ]
    }
   ],
   "source": [
    "# Hole alle DESCRIPTION-Werte (z.B. \"1144-Fatality\", \"1181-Trfc Collision-Minor Inj\", …)\n",
    "descriptions = incidents['DESCRIPTION'].unique()\n",
    "all_results = {}\n",
    "\n",
    "# Für jede DESCRIPTION erfolgt eine separate Analyse\n",
    "for desc in descriptions:\n",
    "    print(f\"\\n--- Analyse für DESCRIPTION: {desc} ---\")\n",
    "    # Filtere Unfälle für die aktuelle DESCRIPTION\n",
    "    desc_incidents = incidents[incidents['DESCRIPTION'] == desc]\n",
    "    all_data_desc = []\n",
    "    \n",
    "    # Gruppiere nach Monat\n",
    "    for month, month_incidents in desc_incidents.groupby('month'):\n",
    "        month_str = f\"p{month.month:02d}\"\n",
    "        if month_str not in MONTHS:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Lade Verkehrsdaten und zugehörige station_ids (hier gilt für beide Sensortypen)\n",
    "            traffic_data = np.load(os.path.join(DATA_PATH, f'{month_str}_car.npy'))\n",
    "            station_ids = np.load(os.path.join(DATA_PATH, f'{month_str}_car_node.npy')).astype(str)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Dateien für {month_str} nicht gefunden\")\n",
    "            continue\n",
    "        \n",
    "        station_to_index = {sid: i for i, sid in enumerate(station_ids)}\n",
    "        \n",
    "        # Finde für den Monat die Vorfälle mit passender Sensor-Kombination (HOV + Mainline)\n",
    "        closest_sensors = find_closest_sensor(month_incidents, sensor_meta)\n",
    "        if closest_sensors.empty:\n",
    "            continue\n",
    "        \n",
    "        # Mapping: Hole den Mainline-Sensor-Index\n",
    "        closest_sensors['mainline_sensor_index'] = closest_sensors['mainline_station_id'].astype(str).map(station_to_index)\n",
    "        valid_sensors = closest_sensors.dropna(subset=['mainline_sensor_index']).copy()\n",
    "        \n",
    "        # Zeitkonvertierung: Verschiebe Zeitpunkt um 5 Minuten zurück und berechne Timeslot\n",
    "        month_start = datetime(month.year, month.month, 1)\n",
    "        valid_sensors['time'] = pd.to_datetime(valid_sensors['time'], errors='coerce') - pd.Timedelta(minutes=5)\n",
    "        valid_sensors['timeslot'] = valid_sensors['time'].apply(lambda x: datetime_to_timeslot(x, month_start))\n",
    "        \n",
    "        time_window = 3  # z. B. 3 Intervalle vor und nach dem Ereignis (jeweils 5 Minuten)\n",
    "        for _, row in valid_sensors.iterrows():\n",
    "            idx = int(row['mainline_sensor_index'])  # Hier werden die Daten des Mainline-Sensors ausgewertet\n",
    "            ts = row['timeslot']\n",
    "    \n",
    "            if ts < 0 or ts >= traffic_data.shape[0]:\n",
    "                continue\n",
    "            \n",
    "            start = max(0, ts - time_window)\n",
    "            end = min(traffic_data.shape[0], ts + time_window + 1)  # +1 für inklusiven Schnitt\n",
    "            \n",
    "            if (end - start) < (2 * time_window):\n",
    "                continue  # Überspringe, falls das Zeitfenster zu klein ist\n",
    "                \n",
    "            traffic_slice = traffic_data[start:end, idx]\n",
    "            if np.isnan(traffic_slice).any():\n",
    "                continue  # Überspringe, wenn NaNs enthalten sind\n",
    "                \n",
    "            all_data_desc.append({\n",
    "                'pre': traffic_slice[:time_window],\n",
    "                'post': traffic_slice[time_window+1:],  # Unfallzeitpunkt überspringen\n",
    "                'feature_names': ['Traffic Volume', 'Occupancy Rate', 'Speed']\n",
    "            })\n",
    "        print(f\"{month_str} für DESCRIPTION {desc} verarbeitet\")\n",
    "    \n",
    "    if len(all_data_desc) == 0:\n",
    "        print(f\"Keine gültigen Daten für DESCRIPTION {desc}\")\n",
    "        continue\n",
    "    \n",
    "    # Aggregiere Pre- und Post-Daten aus allen gültigen Vorfällen\n",
    "    pre_data = []\n",
    "    post_data = []\n",
    "    for entry in all_data_desc:\n",
    "        pre_data.extend(entry['pre'][:])\n",
    "        post_data.extend(entry['post'][:])\n",
    "    \n",
    "    # Optional: Entferne die letzten 3 Elemente aus pre_data, falls vorhanden\n",
    "    if len(pre_data) > 3:\n",
    "        pre_data.pop()\n",
    "        pre_data.pop()\n",
    "        pre_data.pop()\n",
    "    \n",
    "    min_length = min(len(pre_data), len(post_data))\n",
    "    if min_length == 0:\n",
    "        print(f\"Nicht genügend Daten für DESCRIPTION {desc}\")\n",
    "        continue\n",
    "    \n",
    "    stat, p_value = stats.wilcoxon(pre_data[:min_length], post_data[:min_length], alternative='greater')\n",
    "    feature_name = 'Traffic Volume'\n",
    "    all_results[desc] = {\n",
    "        'statistic': stat,\n",
    "        'p_value': p_value,\n",
    "        'median_pre': np.median(pre_data[:min_length]),\n",
    "        'median_post': np.median(post_data[:min_length])\n",
    "    }\n",
    "    \n",
    "    # Daten in numpy-Arrays und Kappen von Extremwerten (z.B. 99%-Quantil)\n",
    "    pre_data = np.array(pre_data[:min_length])\n",
    "    post_data = np.array(post_data[:min_length])\n",
    "    pre_data = np.clip(pre_data, 0, np.percentile(pre_data, 99))\n",
    "    post_data = np.clip(post_data, 0, np.percentile(post_data, 99))\n",
    "    \n",
    "    # --- Boxplot Visualisierung ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.ylim(0, 500)\n",
    "    sns.boxplot(data=[pre_data, post_data], palette=[\"#4c72b0\", \"#dd8452\"])\n",
    "    plt.xticks([0, 1], ['Pre-Incident', 'Post-Incident'])\n",
    "    plt.title(f\"Traffic Volume (Mainline) für {desc}\\np-value: {p_value:.4f}\")\n",
    "    plt.ylabel('Traffic Volume (Normalized)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Violinplot Visualisierung ---\n",
    "    data_plot = pd.DataFrame({\n",
    "        'Traffic Volume': np.concatenate([pre_data, post_data]),\n",
    "        'Period': ['Pre-Incident'] * len(pre_data) + ['Post-Incident'] * len(post_data)\n",
    "    })\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(x='Period', y='Traffic Volume', data=data_plot, palette=[\"#4c72b0\", \"#dd8452\"])\n",
    "    plt.title(f'Verteilung des Mainline Traffic Volume für {desc}')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Traffic Volume (Normalized)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Ausgabe der statistischen Ergebnisse pro DESCRIPTION\n",
    "print(\"\\n=== Statistische Ergebnisse pro DESCRIPTION ===\")\n",
    "for desc, res in all_results.items():\n",
    "    print(f\"\\n{desc}:\")\n",
    "    print(f\"  Wilcoxon-Statistik: {res['statistic']}\")\n",
    "    print(f\"  P-Wert: {res['p_value']:.4f}\")\n",
    "    print(f\"  Median vorher: {res['median_pre']:.2f}\")\n",
    "    print(f\"  Median nachher: {res['median_post']:.2f}\")\n",
    "    print(f\"  Signifikant (p < 0.05): {'Ja' if res['p_value'] < 0.05 else 'Nein'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
